{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqQlGLFP7+h3wvDmJg76cj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/percy-b/Al-kwarizmhi/blob/main/Wikitext_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the save path in your Google Drive\n",
        "save_dir = \"/content/drive/My Drive/model_checkpoints\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(save_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xo-nniovJA6",
        "outputId": "728d751d-a245-49ec-e23e-c6edfcdb46e8",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "T-1b8d6yf7MB"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "import tiktoken\n",
        "\n",
        "import tiktoken\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Initialize tiktoken's GPT-2 BPE encoder.\n",
        "encoder = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def text_to_token_ids(text):\n",
        "    \"\"\"Convert text to token IDs using tiktoken.\"\"\"\n",
        "    return encoder.encode(text)\n",
        "\n",
        "def token_ids_to_text(token_ids):\n",
        "    \"\"\"Convert token IDs back to text using tiktoken.\"\"\"\n",
        "    return encoder.decode(token_ids)\n",
        "\n",
        "# Load a subset of WikiText-103 (for memory efficiency)\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")\n",
        "#dataset = dataset.select(range(20000))  # Use only the first 1000 samples\n",
        "\n",
        "def tokenize_example(example):\n",
        "    # Tokenize the text and store the token IDs in a new field.\n",
        "    example[\"token_ids\"] = text_to_token_ids(example[\"text\"])\n",
        "    return example\n",
        "\n",
        "# Apply tokenization to each example\n",
        "dataset = dataset.map(tokenize_example, batched=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UrX0h3eGf9zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 512  # Set your desired sequence length\n",
        "\n",
        "examples = []\n",
        "for sample in dataset:\n",
        "    tokens = sample[\"token_ids\"]\n",
        "    # Create non-overlapping chunks for this sample.\n",
        "    for i in range(0, len(tokens) - max_seq_length, max_seq_length):\n",
        "        chunk = tokens[i : i + max_seq_length]\n",
        "        examples.append(chunk)\n",
        "\n",
        "print(\"Number of training examples:\", len(examples))\n"
      ],
      "metadata": {
        "id": "ab_rLY41gFSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class WikiTextDataset(Dataset):\n",
        "    def __init__(self, examples):\n",
        "        self.examples = examples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert list of token IDs to a tensor.\n",
        "        input_ids = torch.tensor(self.examples[idx], dtype=torch.long)\n",
        "        # Create targets by shifting the input one token to the left.\n",
        "        targets = input_ids.clone()\n",
        "        targets[:-1] = input_ids[1:]\n",
        "        targets[-1] = -100  # Set the final token to be ignored in loss computation.\n",
        "        return input_ids, targets\n",
        "\n",
        "train_dataset = WikiTextDataset(examples)\n"
      ],
      "metadata": {
        "id": "n5qyouLegLlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n"
      ],
      "metadata": {
        "id": "yF-Yw4lyiVXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    \"\"\"\n",
        "    Generates text using the model, with optional temperature scaling and top-k sampling.\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]  # Focus on last time step\n",
        "\n",
        "        # Top-k filtering\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # Apply temperature scaling if set\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        if eos_id is not None and (idx_next == eos_id).all():\n",
        "            break\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n"
      ],
      "metadata": {
        "id": "t61d25N2gPLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_to_token_ids(prompt):\n",
        "    return torch.tensor([text_to_token_ids(prompt)], dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "-QuYJMM7gVCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# My custom modules:\n",
        "# -------------------------\n",
        "class PositionEmbeddingFixedWeights(nn.Module):\n",
        "    def __init__(self, seq_length, vocab_size, output_dim):\n",
        "        super(PositionEmbeddingFixedWeights, self).__init__()\n",
        "\n",
        "        self.word_embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
        "        self.position_embedding_layer = nn.Embedding(seq_length, output_dim)\n",
        "\n",
        "        pos_embedding_matrix = self.get_position_encoding(seq_length, output_dim)\n",
        "        self.position_embedding_layer.weight.data.copy_(torch.tensor(pos_embedding_matrix, dtype=torch.float))\n",
        "        self.position_embedding_layer.weight.requires_grad = False\n",
        "\n",
        "    def get_position_encoding(self, seq_len, d, n=10000):\n",
        "        P = np.zeros((seq_len, d))\n",
        "        for k in range(seq_len):\n",
        "            for i in np.arange(int(d / 2)):\n",
        "                denominator = np.power(n, 2 * i / d)\n",
        "                P[k, 2 * i] = np.sin(k / denominator)\n",
        "                P[k, 2 * i + 1] = np.cos(k / denominator)\n",
        "        return P\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        position_indices = torch.arange(inputs.size(1), device=inputs.device).unsqueeze(0)\n",
        "        embedded_words = self.word_embedding_layer(inputs)\n",
        "        embedded_positions = self.position_embedding_layer(position_indices)\n",
        "        return embedded_words + embedded_positions\n",
        "\n",
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, queries, keys, values, d_k, mask=None):\n",
        "        scores = queries @ keys.transpose(-2, -1) / torch.sqrt(torch.tensor(d_k, dtype=queries.dtype))\n",
        "        if mask is not None:\n",
        "            scores += -1e9 * mask\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        return weights @ values\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, h, d_model):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.attention = DotProductAttention()\n",
        "        self.heads = h\n",
        "        self.d_head = d_model // h\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def reshape_tensor(self, x, heads, flag):\n",
        "        if flag:\n",
        "            batch_size, seq_length, d_model = x.shape\n",
        "            x = x.reshape(batch_size, seq_length, heads, -1)\n",
        "            x = x.permute(0, 2, 1, 3)\n",
        "        else:\n",
        "            x = x.permute(0, 2, 1, 3)\n",
        "            batch_size, seq_length, heads, depth = x.shape\n",
        "            x = x.reshape(batch_size, seq_length, -1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
        "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
        "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
        "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_head, mask)\n",
        "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
        "        return self.W_o(output)\n",
        "\n",
        "class LeafAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(h, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x, nchunks):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "        segments = x.chunk(nchunks, dim=1)\n",
        "        out = []\n",
        "        for segment in segments:\n",
        "            seg_length = segment.shape[1]\n",
        "            attn_mask = torch.tril(torch.ones(seg_length, seg_length, device=x.device))\n",
        "            m_attn = self.self_attention(segment, segment, segment, mask=attn_mask)\n",
        "            m_attn = self.dropout(self.norm(m_attn))\n",
        "            out.append(m_attn)\n",
        "        concatenated = torch.cat(out, dim=1)\n",
        "        return concatenated\n",
        "\n",
        "class NodeAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.cross_attention = MultiHeadAttention(h, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, left_child, right_child):\n",
        "        queries = right_child\n",
        "        keys = left_child\n",
        "        values = left_child\n",
        "        seq_length = right_child.shape[1]\n",
        "        attn_mask = torch.tril(torch.ones(seq_length, seq_length, device=right_child.device))\n",
        "        m_attn = self.cross_attention(queries, keys, values, mask=attn_mask)\n",
        "        m_attn = self.dropout(self.norm(m_attn))\n",
        "        merged = torch.cat([left_child, m_attn], dim=1)\n",
        "        return merged\n",
        "\n",
        "class HierrachicalAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, levels):\n",
        "        super().__init__()\n",
        "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
        "        self.levels = levels\n",
        "        self.leaf_attention = LeafAttention(h, d_model)\n",
        "        self.node_layers = nn.ModuleList([NodeAttention(h, d_model) for _ in range(levels)])\n",
        "    def forward(self, x):\n",
        "        nchunks = 2 ** self.levels\n",
        "        current = self.leaf_attention(x, nchunks)\n",
        "        for level in range(self.levels):\n",
        "            chunks = current.chunk(2 ** (self.levels - level), dim=1)\n",
        "            next_level = []\n",
        "            for i in range(0, len(chunks), 2):\n",
        "                merged = self.node_layers[level](chunks[i], chunks[i+1])\n",
        "                next_level.append(merged)\n",
        "            current = torch.cat(next_level, dim=1)\n",
        "        return current\n",
        "\n",
        "class HierrachicalDecoderBlock(nn.Module):\n",
        "    def __init__(self, h, d_model, levels):\n",
        "        super().__init__()\n",
        "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
        "        self.hier_attn1 = HierrachicalAttention(h, d_model, levels)\n",
        "        self.hier_attn2 = HierrachicalAttention(h, d_model, levels)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, 4 * d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * d_model, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.levels = levels\n",
        "    def forward(self, x):\n",
        "        original_len = x.shape[1]\n",
        "        nchunks = 2 ** self.levels\n",
        "        padding_size = (nchunks - original_len % nchunks) % nchunks\n",
        "        if padding_size > 0:\n",
        "            x_padded = F.pad(x, (0, 0, 0, padding_size))\n",
        "        else:\n",
        "            x_padded = x\n",
        "        attn1_padded = self.hier_attn1(x_padded)\n",
        "        attn1 = self.norm1(x + attn1_padded[:, :original_len, :])\n",
        "        attn2_padded = self.hier_attn2(F.pad(attn1, (0, 0, 0, padding_size)))\n",
        "        attn2 = self.norm2(attn1 + attn2_padded[:, :original_len, :])\n",
        "        out = self.norm3(attn2 + self.feed_forward(attn2))\n",
        "        return out\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, sequence_length, heads, d_model, levels, drop_out=0.1):\n",
        "        super().__init__()\n",
        "        max_levels = int(math.log2(sequence_length))\n",
        "        if levels > max_levels:\n",
        "            raise ValueError(\n",
        "                f\"For sequence length {sequence_length}, maximum levels is {max_levels} (requested {levels}).\"\n",
        "            )\n",
        "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
        "        self.decoder_block = HierrachicalDecoderBlock(heads, d_model, levels)\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, x, training=True):\n",
        "        pos_encoding_output = self.pos_encoding(x)\n",
        "        x = self.dropout(pos_encoding_output) if training else pos_encoding_output\n",
        "        x = self.decoder_block(x)\n",
        "        logits = self.linear(x)\n",
        "        if training:\n",
        "            return logits\n",
        "        else:\n",
        "            return F.softmax(logits, dim=-1)\n"
      ],
      "metadata": {
        "id": "bUbZk0mggYaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = encoder.n_vocab  # Using tiktoken's vocabulary size\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters for model training\n",
        "vocab_size = encoder.n_vocab  # Using tiktoken's vocabulary size\n",
        "heads = 12\n",
        "d_model = 768        # Embedding dimension (you can adjust)\n",
        "levels = 5\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 100\n",
        "eval_freq = 5  # Generate text every 5 epochs\n",
        "max_new_tokens = 50  # Number of tokens to generate"
      ],
      "metadata": {
        "id": "gHMbCf5uwCpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# First code for training while saving epochs\n",
        "\n",
        "import torch\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters for model training\n",
        "vocab_size = encoder.n_vocab  # Using tiktoken's vocabulary size\n",
        "heads = 12\n",
        "d_model = 768        # Embedding dimension (you can adjust)\n",
        "levels = 5\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 100\n",
        "eval_freq = 2  # Generate text every 5 epochs\n",
        "max_new_tokens = 50  # Number of tokens to generate\n",
        "\n",
        "# Initialize your model (ensure Decoder is defined in your code)\n",
        "model = Decoder(vocab_size, max_seq_length, heads, d_model, levels).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Trainable Parameters: {count_parameters(model):,}\")\n",
        "\n",
        "# Training loop with periodic evaluation & generation\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        inputs, targets = batch\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs, training=True)  # shape: (batch, seq_length, vocab_size)\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Loss: {avg_loss:.4f}\")\n",
        "\n",
        "     # --- Save Model Every 10 Epochs ---\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"✅ Model saved at {save_path}\")\n",
        "\n",
        "\n",
        "    # Every eval_freq epochs, generate text from the fixed prompt.\n",
        "    if (epoch + 1) % eval_freq == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            prompt = \"Every effort moves you\"\n",
        "            start_tokens = prompt_to_token_ids(prompt).to(device)\n",
        "            generated_ids = generate(\n",
        "                model=model,\n",
        "                idx=start_tokens,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                context_size=max_seq_length,\n",
        "                top_k=None,\n",
        "                temperature=0\n",
        "            )\n",
        "            output_text = token_ids_to_text(generated_ids.squeeze().tolist())\n",
        "            print(f\"\\nGenerated Text at Epoch {epoch+1}:\\n{output_text}\\n{'='*50}\\n\")\n",
        "        model.train()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kTAYp7wwgx8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#continue training from checkpoint\n",
        "import torch\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define model save directory\n",
        "save_dir = \"/content/drive/My Drive/model_checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define model hyperparameters\n",
        "vocab_size = encoder.n_vocab  # Using tiktoken's vocabulary size\n",
        "heads = 12\n",
        "d_model = 768        # Embedding dimension (you can adjust)\n",
        "levels = 5\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 100\n",
        "eval_freq = 2  # Generate text every 2 epochs\n",
        "max_new_tokens = 50  # Number of tokens to generate\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = Decoder(vocab_size, max_seq_length, heads, d_model, levels).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "# Find the latest checkpoint\n",
        "checkpoint_files = sorted(glob(os.path.join(save_dir, \"model_epoch_*.pth\")), key=os.path.getmtime)\n",
        "latest_checkpoint = checkpoint_files[-1] if checkpoint_files else None\n",
        "start_epoch = 0\n",
        "\n",
        "# Load checkpoint if available\n",
        "if latest_checkpoint:\n",
        "    print(f\"🔄 Loading checkpoint from {latest_checkpoint}\")\n",
        "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
        "    model.load_state_dict(checkpoint)\n",
        "    start_epoch = int(latest_checkpoint.split(\"_\")[-1].split(\".\")[0])  # Extract epoch number\n",
        "    print(f\"✅ Resumed training from epoch {start_epoch}\")\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(start_epoch, 300):\n",
        "    epoch_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        inputs, targets = batch\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs, training=True)\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save model every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"✅ Model saved at {save_path}\")\n",
        "\n",
        "    # Generate text every eval_freq epochs\n",
        "    if (epoch + 1) % eval_freq == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            prompt = \"Every effort moves you\"\n",
        "            start_tokens = prompt_to_token_ids(prompt).to(device)\n",
        "            generated_ids = generate(\n",
        "                model=model,\n",
        "                idx=start_tokens,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                context_size=max_seq_length,\n",
        "                top_k=20,\n",
        "                temperature=0.3\n",
        "            )\n",
        "            output_text = token_ids_to_text(generated_ids.squeeze().tolist())\n",
        "            print(f\"\\nGenerated Text at Epoch {epoch+1}:\\n{output_text}\\n{'='*50}\\n\")\n",
        "        model.train()\n"
      ],
      "metadata": {
        "id": "k5kDE7_J8mBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YAyfwGi6YVU0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}